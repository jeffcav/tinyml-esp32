{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with floating point weights and baremetal C implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import serial\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import src.python.dataset.yalefaces as yalefaces\n",
    "import src.python.model.util as util\n",
    "\n",
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "### Load and normalize the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = yalefaces.load(\"dataset/yalefaces\", flatten=True)\n",
    "X = X.astype(\"float32\") / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress dataset with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_faces, num_pixels = X.shape\n",
    "num_principal_components = int(num_faces)\n",
    "pca = PCA(n_components=num_principal_components)\n",
    "\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert datasets to pythorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train))\n",
    "test_dataset = TensorDataset(torch.Tensor(X_test), torch.LongTensor(y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = torch.nn.Sequential(\n",
    "      torch.nn.Linear(165, 96, bias=True),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Linear(96, 15, bias=True),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs= []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_data = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    error, num_samples = util.train(model, device, train_data, optimizer)\n",
    "    loss = float(error)/float(num_samples)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    acc = util.test(model, device, train_data)\n",
    "    train_accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.82\n"
     ]
    }
   ],
   "source": [
    "test_data = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "acc = util.test(model, device, test_data)\n",
    "print(f\"Test accuracy: {acc * 100:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=165, out_features=96, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=96, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy our original model\n",
    "qmodel_float = copy.deepcopy(model.layers)\n",
    "qmodel_float.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse layers (weights+activation)\n",
    "torch.quantization.fuse_modules(qmodel_float, ['0', '1'], inplace=True)\n",
    "\n",
    "# add quantization of input and output\n",
    "qmodel_float = torch.nn.Sequential(\n",
    "    torch.quantization.QuantStub(),\n",
    "    *qmodel_float,\n",
    "    torch.quantization.DeQuantStub()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure quantization\n",
    "qmodel_float.qconfig = torch.quantization.default_qconfig\n",
    "qmodel_float=qmodel_float.to('cpu')\n",
    "qmodel_float.qconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (1): LinearReLU(\n",
       "    (0): Linear(in_features=165, out_features=96, bias=True)\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (2): Identity()\n",
       "  (3): Linear(\n",
       "    in_features=96, out_features=15, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (4): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize quantization parameters\n",
    "torch.quantization.prepare(qmodel_float, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn quantization parameters from test samples\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (x, y) in enumerate(test_data):\n",
    "        x,y = x.to('cpu'), y.to('cpu')\n",
    "        qmodel_float(x)\n",
    "\n",
    "# quantize weights\n",
    "qmodel = torch.quantization.convert(qmodel_float, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight size before quantization: 4 byte(s)\n",
      "Weight size after quantization: 1 byte(s)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight size before quantization:\", qmodel_float[1][0].weight.element_size(), \"byte(s)\")\n",
    "print(\"Weight size after quantization:\", qmodel[1].weight().element_size(), \"byte(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.82\n"
     ]
    }
   ],
   "source": [
    "# measure accuracy of the quantized model\n",
    "test_data = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "acc = util.test(qmodel, 'cpu', test_data)\n",
    "print(f\"Test accuracy: {acc * 100:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model weights as C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = model.state_dict()\n",
    "qmodel_params = qmodel.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_indexes = [1, 3]\n",
    "\n",
    "src_filename = 'src/embedded/2-mlp-baremetal-int8/esp32s3/main/mlp_weights.c'\n",
    "hdr_filename = 'src/embedded/2-mlp-baremetal-int8/esp32s3/include/mlp_weights.h'\n",
    "\n",
    "with open(src_filename, 'w') as source, open(hdr_filename, 'w') as header:\n",
    "\n",
    "    # header preamble\n",
    "    header.write('#ifndef MLP_WEIGHTS\\n#define MLP_WEIGHTS\\n\\n')\n",
    "    header.write('#include <stdint.h>\\n\\n')\n",
    "\n",
    "    # source includes\n",
    "    source.write('#include \"mlp_weights.h\"\\n\\n')\n",
    "\n",
    "    # input: quantization params\n",
    "    x_scale, x_zero = util.get_input_qparams(qmodel_params)\n",
    "    header.write(f\"extern const float input_zero;\\n\")\n",
    "    header.write(f\"extern const float input_scale;\\n\\n\")\n",
    "    source.write(f\"const float input_zero = {x_zero};\\n\")\n",
    "    source.write(f\"const float input_scale = {x_scale};\\n\\n\")\n",
    "\n",
    "    for layer in layer_indexes:\n",
    "        weights = util.get_weights(model_params, layer-1).flatten()\n",
    "        weights_scale, layer_scale = util.get_scale(qmodel_params, layer)\n",
    "        weights_zero, layer_zero = util.get_zero(qmodel_params, layer)\n",
    "\n",
    "        # quantize weights and bias\n",
    "        qweights = (np.around(weights / weights_scale) + weights_zero).astype(int)\n",
    "        qweights -= weights_zero\n",
    "\n",
    "        # layer and weights: quantization params\n",
    "        header.write(f\"extern const int8_t layer_{layer}_weights_zero;\\n\")\n",
    "        header.write(f\"extern const float layer_{layer}_weights_scale;\\n\\n\")\n",
    "        header.write(f\"extern const int8_t layer_{layer}_zero;\\n\")\n",
    "        header.write(f\"extern const float layer_{layer}_scale;\\n\\n\")\n",
    "\n",
    "        source.write(f\"const int8_t layer_{layer}_weights_zero = {weights_zero};\\n\")\n",
    "        source.write(f\"const float layer_{layer}_weights_scale = {weights_scale};\\n\\n\")\n",
    "        source.write(f\"const int8_t layer_{layer}_zero = {layer_zero};\\n\")\n",
    "        source.write(f\"const float layer_{layer}_scale = {layer_scale};\\n\\n\")\n",
    "\n",
    "        # Weights\n",
    "        header.write(f\"extern const int8_t layer_{layer}_weights[{len(weights)}];\\n\")\n",
    "        source.write(f\"const int8_t layer_{layer}_weights[{len(weights)}] = {{\")\n",
    "        for i in range(len(weights)-1):\n",
    "            source.write(f\"{qweights[i]}, \")\n",
    "        source.write(f\"{qweights[len(qweights)-1]}}};\\n\\n\")\n",
    "\n",
    "    header.write('\\n#endif // end of MLP_PARAMS\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk to esp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 84.85%, Average Inference Duration = 1.458ms: 100%|██████████| 33/33 [00:02<00:00, 14.67it/s]\n"
     ]
    }
   ],
   "source": [
    "CPU_FREQ_KHZ = 240000\n",
    "num_tests = len(X_test)\n",
    "\n",
    "with serial.Serial(\"/dev/ttyUSB0\", baudrate=115200, timeout=None) as esp32, tqdm(total=num_tests, file=sys.stdout) as pbar:\n",
    "    esp32.read_until(b'Ready\\n')\n",
    "    \n",
    "    num_correct = 0\n",
    "    all_elapsed = []\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        face = X_test[i]\n",
    "        expected = y_test[i]\n",
    "\n",
    "        # msg = esp32.read_until(b'Waiting for input\\n')\n",
    "        expected_msg = b'Waiting for input\\n'\n",
    "        msg = esp32.read_until(expected_msg)\n",
    "        \n",
    "        # msg = esp32.read(len(expected_msg))\n",
    "        assert msg == expected_msg, msg\n",
    "\n",
    "        # send command=1 (new inference)\n",
    "        esp32.write(b'\\x01')\n",
    "\n",
    "        # send input (165 bytes)\n",
    "        esp32.write(face.tobytes())\n",
    "\n",
    "        # read output\n",
    "        subject = esp32.read(4)\n",
    "        subject = int.from_bytes(subject, byteorder=\"little\")\n",
    "\n",
    "        # read inference duration\n",
    "        elapsed = esp32.read(4)\n",
    "        elapsed = int.from_bytes(elapsed, byteorder=\"little\")\n",
    "        all_elapsed.append(elapsed)\n",
    "\n",
    "        # count correct inferences\n",
    "        if expected == subject:\n",
    "            num_correct += 1\n",
    "\n",
    "        # print status\n",
    "        acc = num_correct/(i+1)\n",
    "        pbar.set_description(f\"Accuracy = {acc*100:.2f}%, Average Inference Duration = {elapsed/CPU_FREQ_KHZ:.3f}ms\")\n",
    "        pbar.update(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "335bc577f4714844361dc0df4b7a2ea152f61cb4ad8245b2534b0c0899fbb0f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
